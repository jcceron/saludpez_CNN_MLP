{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc2501a3",
   "metadata": {},
   "source": [
    "# Clasificación Multimodal de Salud de Peces (En desarrollo)\n",
    "\n",
    "Este notebook cubre:\n",
    "\n",
    "1. División estratificada en entrenamiento y prueba  \n",
    "2. Evaluación del modelo (Accuracy, Precision, Recall, F1, Matriz de Confusión, Reporte de Clasificación)  \n",
    "3. Serialización del mejor checkpoint y función de inferencia  \n",
    "4. Entrenamiento con EarlyStopping y ReduceLROnPlateau  \n",
    "5. Búsqueda de hiperparámetros (lr, tamaño de batch, arquitectura MLP)  \n",
    "6. Despliegue con FastAPI  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2902b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Agrega la raíz del proyecto al path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd622ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importaciones de librerías\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import joblib\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from pydantic import BaseModel\n",
    "import uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b9eb0",
   "metadata": {},
   "source": [
    "### Cargue de datos preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f438b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Carga y preprocesado de datos multimodales ---\n",
    "\n",
    "# --- Configuración de rutas y nombres de columna ---\n",
    "sensor_cols    = ['temperatura','pH','conductividad','TDS','DO_mgL']\n",
    "label_col      = 'etiqueta_kmeans'\n",
    "preproc_dir    = \"../data/images_preproc\"\n",
    "label_csv      = \"../data/labels2_kmeans_limpio.csv\"\n",
    "nombre_img_col = 'imagen'   \n",
    "\n",
    "# 1. Lee el CSV y codifica etiquetas de texto a enteros\n",
    "df = pd.read_csv(label_csv)\n",
    "le = LabelEncoder()\n",
    "df['label_enc'] = le.fit_transform(df[label_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DIVISIÓN ESTRATIFICADA\n",
    "# 1. DIVISIÓN ESTRATIFICADA\n",
    "def split_dataset(dataset, \n",
    "                  labels, \n",
    "                  test_size=0.2, \n",
    "                  val_size=0.1, \n",
    "                  random_state=42):\n",
    "    \"\"\"Divide el dataset en train/val/test estratificado.\"\"\"\n",
    "    # — sss1: primer splitter para separar test del resto (train+val)\n",
    "    sss1 = StratifiedShuffleSplit(\n",
    "        n_splits=1,              # una única división\n",
    "        test_size=test_size,     # proporción destinada a test (p. ej. 0.2 → 20%)\n",
    "        random_state=random_state\n",
    "    )\n",
    "    # next(...) devuelve los índices para train+val y para test\n",
    "    #   .split(X, y) necesita un X cualquiera y las etiquetas y;\n",
    "    #   aquí X es sólo un array de ceros con la longitud de labels\n",
    "    idx_train_val, idx_test = next(\n",
    "        sss1.split(\n",
    "            np.zeros(len(labels)),  # dummy array, sólo importan las etiquetas\n",
    "            labels                  # etiquetas originales para stratify\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Extraemos las etiquetas de train+val para el siguiente split\n",
    "    labels_train_val = labels[idx_train_val]\n",
    "\n",
    "    # — sss2: segundo splitter para separar train y val dentro del resto\n",
    "    #    La proporción de validación relativa es val_size/(1-test_size)\n",
    "    sss2 = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        test_size=val_size/(1-test_size),\n",
    "        random_state=random_state\n",
    "    )\n",
    "    # next(...) ahora divide indices de train_val en train vs val\n",
    "    idx_train, idx_val = next(\n",
    "        sss2.split(\n",
    "            np.zeros(len(labels_train_val)),  # de nuevo, dummy X\n",
    "            labels_train_val                  # etiquetas de train+val\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Mapear de vuelta a los índices originales del dataset\n",
    "    train_idx = idx_train_val[idx_train]\n",
    "    val_idx   = idx_train_val[idx_val]\n",
    "\n",
    "    # Devolver tres Subsets de PyTorch: train, val y test\n",
    "    return Subset(dataset, train_idx), \\\n",
    "           Subset(dataset, val_idx), \\\n",
    "           Subset(dataset, idx_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d671d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso:\n",
    "# train_ds, val_ds, test_ds = split_dataset(full_dataset, full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b983792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DATALOADERS\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fabe599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MODELO: definimos MLP multimodal ejemplo\n",
    "class MLPMultimodal(nn.Module):\n",
    "    def __init__(self, input_img_dim, input_sensor_dim, n_classes):\n",
    "        super().__init__()\n",
    "        # encoder de imagen (CNN sencilla)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        # calculamos tamaño de salida de la CNN\n",
    "        dummy = torch.zeros(1,3,*input_img_dim)\n",
    "        img_feat_dim = self.cnn(dummy).shape[1]\n",
    "        # encoder de sensores (MLP)\n",
    "        self.mlp_sens = nn.Sequential(\n",
    "            nn.Linear(input_sensor_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 32), nn.ReLU()\n",
    "        )\n",
    "        # capa final combinada\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(img_feat_dim + 32, 128), nn.ReLU(),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, sens):\n",
    "        f1 = self.cnn(img)\n",
    "        f2 = self.mlp_sens(sens)\n",
    "        x  = torch.cat([f1, f2], dim=1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee30ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ENTRENAMIENTO CON EarlyStopping y ReduceLROnPlateau\n",
    "class EarlyStopping:\n",
    "    \"\"\"Detiene el entrenamiento si la métrica de validación no mejora tras 'patience' épocas.\"\"\"\n",
    "    def __init__(self, patience=5, min_delta=1e-4):\n",
    "        self.patience  = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = np.inf\n",
    "        self.counter   = 0\n",
    "        self.should_stop = False\n",
    "\n",
    "    def step(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter   = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "    earlystop = EarlyStopping(patience=7)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, 101):\n",
    "        # --- entrenamiento ---\n",
    "        model.train()\n",
    "        for imgs, sens, labels in train_loader:\n",
    "            imgs, sens, labels = imgs.to(device), sens.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(imgs, sens)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # --- validación ---\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, sens, labels in val_loader:\n",
    "                imgs, sens, labels = imgs.to(device), sens.to(device), labels.to(device)\n",
    "                out = model(imgs, sens)\n",
    "                val_losses.append(criterion(out, labels).item())\n",
    "        val_loss = np.mean(val_losses)\n",
    "        print(f\"Época {epoch}: val_loss = {val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        earlystop.step(val_loss)\n",
    "        if earlystop.should_stop:\n",
    "            print(\"EarlyStopping: se detiene entrenamiento.\")\n",
    "            break\n",
    "\n",
    "    # guardamos mejor checkpoint\n",
    "    torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1333415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. EVALUACIÓN EN TEST\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.to(device).eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, sens, labels in test_loader:\n",
    "            imgs, sens  = imgs.to(device), sens.to(device)\n",
    "            out = model(imgs, sens)\n",
    "            preds = out.argmax(dim=1).cpu().numpy()\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(labels.numpy())\n",
    "    # Métricas\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "    print(\"Matriz de confusión:\\n\", cm)\n",
    "    print(\"Reporte de clasificación:\\n\", classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f283dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. FUNCIÓN DE INFERENCIA\n",
    "class Inferencia:\n",
    "    def __init__(self, model_path, input_img_dim, input_sensor_dim, n_classes, device):\n",
    "        self.device = device\n",
    "        self.model  = MLPMultimodal(input_img_dim, input_sensor_dim, n_classes)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        self.model.to(device).eval()\n",
    "        # transformaciones para la imagen\n",
    "        self.tx = transforms.Compose([\n",
    "            transforms.Resize(input_img_dim),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                                 std =[0.229,0.224,0.225])\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img_bytes, sens_array):\n",
    "        # procesar imagen\n",
    "        img = Image.open(img_bytes).convert(\"RGB\")\n",
    "        img = self.tx(img).unsqueeze(0).to(self.device)\n",
    "        # procesar sensores\n",
    "        sens = torch.tensor(sens_array, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        # inferencia\n",
    "        with torch.no_grad():\n",
    "            out = self.model(img, sens)\n",
    "            probs = torch.softmax(out, dim=1).cpu().numpy()[0]\n",
    "            cls   = int(probs.argmax())\n",
    "        return {\"clase\": cls, \"probabilidades\": probs.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65975d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. BÚSQUEDA DE HIPERPARÁMETROS (ejemplo sencillo)\n",
    "def hyperparameter_search(param_grid, dataset, labels, device):\n",
    "    mejores = {}\n",
    "    for lr in param_grid[\"lr\"]:\n",
    "        for bs in param_grid[\"batch_size\"]:\n",
    "            for hid in param_grid[\"hidden\"]:\n",
    "                # redefinir dataset y loaders\n",
    "                train_ds, val_ds, _ = split_dataset(dataset, labels)\n",
    "                tl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "                vl = DataLoader(val_ds,   batch_size=bs, shuffle=False)\n",
    "                # crear modelo con hidden=h\n",
    "                model = MLPMultimodal(input_img_dim=(64,64),\n",
    "                                      input_sensor_dim=labels.shape[1],\n",
    "                                      n_classes=3)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "                # corto entrenamiento por N épocas o con early stop\n",
    "                # ...\n",
    "                # evaluar en val, guardar mejores\n",
    "                val_acc = ...  # tu lógica de evaluación\n",
    "                llave = (lr, bs, hid)\n",
    "                mejores[llave] = val_acc\n",
    "    return max(mejores, key=mejores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ac4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. DESPLIEGUE CON FastAPI\n",
    "app = FastAPI()\n",
    "class Sensors(BaseModel):\n",
    "    datos: list  # lista de floats\n",
    "\n",
    "# instancia global de inferencia\n",
    "infer = Inferencia(\"best_model.pt\", (64,64), input_sensor_dim=10, n_classes=3, device=\"cpu\")\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(file: UploadFile = File(...), sensors: Sensors = None):\n",
    "    resultado = infer(file.file, sensors.datos)\n",
    "    return resultado\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
